{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import CONST\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "\n",
    "from entities import WoundDataset\n",
    "from models.trainer.diffuser import Diffuser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mqhuy0168\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /Users/nathanbui/.netrc\n"
     ]
    }
   ],
   "source": [
    "# Login wandb\n",
    "wandb_key = \"a8b5a7676a58d9b5b1e686fd9d349bc25f18d07c\"\n",
    "wand_logged = wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'bayes',\n",
      " 'metric': {'goal': 'minimize', 'name': 'train_mse_loss'},\n",
      " 'parameters': {'attn_heads': {'values': [1, 2, 4]},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.0003,\n",
      "                                  'min': 5e-05},\n",
      "                'noise_steps': {'values': [50, 100, 500, 1000]},\n",
      "                'variance_schedule_type': {'values': ['linear',\n",
      "                                                      'quadratic',\n",
      "                                                      'sigmoid',\n",
      "                                                      'cosine']}},\n",
      " 'project': 'DDPM_hyper_tuning'}\n"
     ]
    }
   ],
   "source": [
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    \"project\": \"DDPM_hyper_tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"train_mse_loss\",\n",
    "        \"goal\": \"minimize\",\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 5e-5, \"max\": 3e-4},\n",
    "        \"noise_steps\": {\"values\": [50, 100, 500, 1000]},\n",
    "        \"variance_schedule_type\": {\"values\": [\"linear\", \"quadratic\", \"sigmoid\", \"cosine\"]},\n",
    "        \"attn_heads\": {\"values\": [1, 2, 4]}\n",
    "    }\n",
    "}\n",
    "\n",
    "pprint(sweep_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7saxy261\n",
      "Sweep URL: https://wandb.ai/qhuy0168/DDPM_hyper_tuning/sweeps/7saxy261\n"
     ]
    }
   ],
   "source": [
    "# Initialise sweep controller on WandB server\n",
    "# After initialised, we do not need to run this again\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DDPM_hyper_tuning\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7saxy261\n"
     ]
    }
   ],
   "source": [
    "print(sweep_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define the agent\n",
    "# Run this on agent computers\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Training trigger for hyper-parameter optimisation\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Init the run\n",
    "    run = wandb.init(project=\"DDPM_hyper_tuning\")\n",
    "\n",
    "    # Get the configuration for instance\n",
    "    config = wandb.config\n",
    "\n",
    "    # Similar approach to full train\n",
    "    # The data\n",
    "    dataset = WoundDataset(\n",
    "        image_dir=CONST.PROCESSED_IMAGES_DIR,\n",
    "        segment_dir=CONST.PROCESSED_SEGMENT_DIR,\n",
    "        target_tensor_size=CONST.DIFFUSER_SETTINGS.INPUT_SIZE,\n",
    "        embedding_dir=CONST.PROCESSED_EMBEDDING_DIR\n",
    "    )\n",
    "\n",
    "    diffuser = Diffuser(\n",
    "        dataset=dataset,\n",
    "        batch_size=8,\n",
    "        num_workers=2,\n",
    "        epochs=500,\n",
    "        run_name=CONST.DIFFUSER_SETTINGS.RUN_NAME,\n",
    "        output_dir=CONST.DIFFUSER_SETTINGS.OUTPUT_DIR,\n",
    "        max_lr=config.learning_rate,\n",
    "        noise_steps=config.noise_steps,\n",
    "        variance_schedule_type=config.variance_schedule_type,\n",
    "        attn_heads=config.attn_heads,\n",
    "        wandb_run=run,\n",
    "    )\n",
    "\n",
    "    diffuser.fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start the sweep agent\n",
    "wandb.agent(sweep_id=\"7saxy261\", project=\"DDPM_hyper_tuning\", function=train, count=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
